#!/usr/bin/python3

import sys
import socket
import ssl
from ssl import SSLContext
from bs4 import BeautifulSoup
import warnings

# Ignore DeprecationWarning
warnings.filterwarnings('ignore', category=DeprecationWarning)

def http_cache(response):
    # Make soup object
    soup = BeautifulSoup(response, 'html.parser')

    # Save the last response in a file
    with open("cache", "w") as file:
        file.write(soup.prettify())


def return_content(response):
    # Decode and print the response
    soup = BeautifulSoup(response, 'html.parser')

    # Extract content from the response
    content = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'a'])

    # Print the content
    for tag in content:
        if tag.name == 'h1' or tag.name == 'h2' or tag.name == 'h3' or tag.name == 'h4' or tag.name == 'h5' or tag.name == 'h6':
            print("\n", end="")
            print(tag.get_text().strip())
            print("\n", end="")
   
        elif tag.name == 'ul':
            for li in tag.find_all('li'):
                print("-", li.get_text().strip())
        
        elif tag.name == 'a':
            if tag.get('href')[:4] == 'http' or tag.get('href')[:5] == 'https':
                print(tag.get_text().strip())
                print("->", tag.get('href'))
            else:
                pass

        elif tag.name == 'p':
            print(tag.get_text().strip())    

def return_search_results(response):
    # Get top 10 search results
    soup = BeautifulSoup(response, 'html.parser')
    results = soup.find_all('li', class_='b_algo')

    # Print the search results
    for i, result in enumerate(results):
        if i == 10:
            break
        else:
            print(f"{i+1}. {result.h2.get_text()}")
            link = result.find('cite')
            print(f"   {link.get_text()}")
            description = result.find('p')
            print(f"   {description.get_text()}\n")

def make_http_request(url):
    # Parse the URL to get host and path
    if 'http://' in url:
        url = url.replace('http://', '', 1)
    elif 'https://' in url:
        url = url.replace('https://', '', 1)

    parts = url.split('/', 1) # Split the URL into two parts
    host = parts[0] # Get the host
    path = '/' + parts[1] if len(parts) > 1 else '/' # Get the path

    # Create a socket object
    client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    # Connect to the server
    client_socket.connect((host, 80))

    # Construct the HTTP request
    request = f"GET {path} HTTP/1.1\r\nHost:{host}\r\nConnection: close\r\n\r\n"

    # Send the request
    client_socket.sendall(request.encode("utf-8"))

    # Receive the response
    response = b''
    while True:
        data = client_socket.recv(1024)
        if not data:
            break
        response += data

    # Close the socket
    client_socket.close()

    # Decode and print the response
    # print(response.decode("latin-1"))

    # Parse the response using BeautifulSoup
    soup = BeautifulSoup(response, 'html.parser')

    # print(soup.prettify())
    
    # Check if the response is a redirect
    if soup.decode().startswith("HTTP/1.1 301") or soup.decode().startswith("HTTP/1.1 302") or soup.decode().startswith("HTTP/1.1 303") or soup.decode().startswith("HTTP/1.1 307") or soup.decode().startswith("HTTP/1.1 308"):

        print("Redirecting...\n")
        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        ssl_socket = ssl.wrap_socket(client_socket, ssl_version=ssl.PROTOCOL_TLS)
        ssl_socket.connect((host, 443))
        ssl_socket.sendall(request.encode("utf-8"))

        response = b''
        while True:
            data = ssl_socket.recv(1024)
            if not data:
                break
            response += data


        ssl_socket.close()
    
    return response

def main():
    # Check if the correct number of arguments is provided
    if len(sys.argv) > 5 or len(sys.argv) < 1 or sys.argv[1] == "-h":
        print("Help:\n    go2web -u <URL>         # make an HTTP request to the specified URL and print the response \n    go2web -s <search-term> # make an HTTP request to search the term using your favorite search engine and print top 10 results \n    go2web -h               # show this help \n")
        return
    

    if sys.argv[1] == "-u" and len(sys.argv) == 3:
        # Get the URL from the command-line argument
        url = sys.argv[2]

        # Make the HTTP request
        content = make_http_request(url)

        # Print the content
        return_content(content)

        return

    elif sys.argv[1] == "-s" and len(sys.argv) == 3:
        # Get the search term from the command-line argument
        search_term = sys.argv[2]
        search_term = search_term.replace(" ", "+")
        print(f"Searching for: {search_term}")

        # Get the last search term from last_term.txt
        last_term = ""
        with open("last_term.txt", "r") as file:
            last_term = file.read()

        if search_term == last_term:
            # Get the last response from the cache file
            with open("cache", "r") as file:
                results = file.read()
        
        else:
        # Save the last search term in a file
            with open("last_term.txt", "w") as file:
                file.write(search_term)

            # Make the HTTP request
            results = make_http_request(f"https://www.bing.com/search?q={search_term}")

            # Save the last response in a file
            http_cache(results)

        # Print the search results
        return_search_results(results)

        return

    elif sys.argv[1] == '-u' and sys.argv[3] == '-s' and len(sys.argv) == 5:
        # Get the URL from the command-line argument
        url = sys.argv[2]

        # Get the search term from the command-line argument
        search_term = sys.argv[4]
        search_term = search_term.replace(" ", "+")
        print(f"Searching for: {search_term}")

        # Make the HTTP request
        results = make_http_request(f"{url}/search?q={search_term}")

        # Print the search results
        return_content(results)

        # Save the last response in a file
        # http_cache(results)

        return
    
    # # Access a link in the search results
    # elif sys.argv[1] == '-a' and len(sys.argv) == 3:
    #     # Get the link number from the command-line argument
    #     link_number = int(sys.argv[2])

    #     print(f"Accessing link number {link_number}...\n")

    #     # Get the last response from the cache file
    #     with open("cache.txt", "r") as file:
    #         results = file.read()

    #     # Parse the response using BeautifulSoup
    #     soup = BeautifulSoup(results, 'html.parser')

    #     # Get the search results
    #     search_results = soup.find_all('li', class_='b_algo')
        
    #     # Get the link
    #     link = search_results[link_number - 1].find('a')

    #     # Get the URL
    #     url = link.get('href')
    #     print(f"URL: {url}")

    #     # Make the HTTP request
    #     content = make_http_request(url)

    #     # Print the content
    #     return_content(content)

    #     return
    

if __name__ == "__main__":
    main()
